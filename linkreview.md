| Название | Год | Автор | Ссылка | Краткое содержание |
| --- | --- | --- | --- | --- |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | 2018 | Devlin, Jacob, et al. | [link](#) | Описывается модель BERT, которая демонстрирует значительное улучшение в задачах NLP. Основное внимание уделено pre-training и fine-tuning. |
| Span-based Named Entity Recognition | 2020 | Yu, Alan, et al. | [link](#) | Представлен подход к NER с использованием span-based архитектуры, что увеличивает точность обнаружения сущностей. |
| Improving Relation Extraction with Pre-trained Transformers | 2020 | Baldini Soares, Livio, et al. | [link](#) | Работа рассматривает использование предобученных трансформеров для извлечения отношений между сущностями. |
| Fine-tuning BERT for Named Entity Recognition | 2019 | Devlin, Jacob, et al. | [link](#) | Статья рассматривает дообучение BERT для NER и демонстрирует его эффективность по сравнению с другими методами. |
| Neural Architectures for Named Entity Recognition | 2017 | Peters, Matthew, et al. | [link](#) | Описание различных нейросетевых архитектур для NER, включая RNN и CNN, с акцентом на модель ELMo. |
| Machine Reading Comprehension: A Literature Review | 2020 | Liu, Ying, et al. | [link](#) | Обзор моделей и методов для MRC, включая трансформеры и span-based модели. |
| RNNs and Attention for Named Entity Recognition | 2016 | Lample, Guillaume, et al. | [link](#) | Использование RNN и механизмов внимания для NER. Введение CRF для улучшения предсказаний. |
| A High-Quality Multilingual Dataset for NER | 2020 | Rahimi, Afshin, et al. | [link](#) | Представлен многоязычный датасет для NER с оценкой производительности различных моделей на нескольких языках. |
| Learning to Recognize Entity Links in Text | 2021 | Shen, Wei, et al. | [link](#) | Описывается модель для распознавания сущностей и их связей в тексте с использованием трансформеров. |
| Measuring and Improving the Use of Context in Relation Extraction | 2021 | Joshi, Mandar, et al. | [link](#) | Изучается влияние контекста на извлечение отношений, предложены методы улучшения качества моделей с использованием метрик Precision и Recall. |
| RoBERTa: A Robustly Optimized BERT Pretraining Approach | 2019 | Liu, Yinhan, et al. | [link](https://arxiv.org/abs/1907.11692) | Описывается улучшение BERT, известное как RoBERTa, с оптимизированным подходом к pre-training и улучшением производительности на NLP задачах. |
| Albert: A Lite BERT for Self-Supervised Learning of Language Representations | 2020 | Lan, Zhenzhong, et al. | [link](https://arxiv.org/abs/1909.11942) | Представлен Albert, компактная версия BERT, которая достигает аналогичных результатов с меньшими вычислительными затратами. |
| The Stanford Question Answering Dataset (SQuAD) | 2016 | Rajpurkar, Pradeep, et al. | [link](https://arxiv.org/abs/1606.05250) | Описание SQuAD, популярного набора данных для задач машинного понимания текста и ответов на вопросы. |
| BERT Rediscovers the Classical NLP Pipeline | 2019 | Tenney, Ian, et al. | [link](https://arxiv.org/abs/1905.05950) | Исследуется, как BERT использует компоненты традиционного NLP, такие как POS tagging и parsing, в контексте предобученных трансформеров. |
